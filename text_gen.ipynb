{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_gen.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO7ejniUwLGNT6vN0SRRhKr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JonasMarma/TG-Eng-Info-UFABC/blob/main/text_gen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBPKg0iwe6oL"
      },
      "source": [
        "#Bibliografia:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qC0YqBpe4ws"
      },
      "source": [
        "https://www.tensorflow.org/text/tutorials/text_generation\n",
        "\n",
        "Ver as notas com estrelinhas para sugestões de como melhorar o código!\n",
        "\n",
        "Ideias minhas:\n",
        "\n",
        "E se fizer com que o vocabulário seja composto por palavras?\n",
        "\n",
        "Acho que só isso kkkj"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtNKNzupknGg"
      },
      "source": [
        "**If you want the model to generate text faster the easiest thing you can do is batch the text generation. In the example below the model generates 5 outputs in about the same time it took to generate 1 above.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrF1yHbpfp9T"
      },
      "source": [
        "# Preparação dos dados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FG68bCtePAF6"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6TtPBrePFtm",
        "outputId": "31ad8a85-e39e-4527-d674-7c970fdb9691"
      },
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "\n",
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')\n",
        "\n",
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n",
            "1130496/1115394 [==============================] - 0s 0us/step\n",
            "Length of text: 1115394 characters\n",
            "65 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKhM1J_fPoyX"
      },
      "source": [
        "# Criação da camada de transformação: char -> id\n",
        "ids_from_chars = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
        "    vocabulary=list(vocab),\n",
        "    mask_token=None)\n",
        "\n",
        "# Criação da camada de transformação: id -> char\n",
        "chars_from_ids = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(),\n",
        "    invert=True,\n",
        "    mask_token=None)\n",
        "\n",
        "# Função para transformar ids diretamente em textos\n",
        "# https://www.tensorflow.org/api_docs/python/tf/strings/reduce_join\n",
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fE4zHAj_Qj_V",
        "outputId": "c47d5519-5802-4677-e4bb-ae33abbc0eef"
      },
      "source": [
        "# Separar o texto em caracteres\n",
        "all_chars = tf.strings.unicode_split(text, 'UTF-8')\n",
        "\n",
        "# Passar todos esses caracteres pela camada de conversão para ids\n",
        "all_ids = ids_from_chars(all_chars)\n",
        "print(all_ids)\n",
        "\n",
        "# Converter essa sequência de ids em um dataset\n",
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "print(ids_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([19 48 57 ... 46  9  1], shape=(1115394,), dtype=int64)\n",
            "<TensorSliceDataset shapes: (), types: tf.int64>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkKJz8nFT2Fa"
      },
      "source": [
        "# Para cada exemplo do treinamento, utilizar uma sequência de 100 caracteres\n",
        "seq_length = 100\n",
        "# O número de exemplos a cada época é tam_texto/tam_seq\n",
        "examples_per_epoch = len(text)//(seq_length+1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hURCI_5UWvJE",
        "outputId": "d1926906-f93b-428e-e3a8-25b031ebbb99"
      },
      "source": [
        "# Criar o batch de sequências\n",
        "seq_length = 100\n",
        "\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "print(sequences)\n",
        "\n",
        "# Mostrar 5 exemplos\n",
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<BatchDataset shapes: (101,), types: tf.int64>\n",
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrEqBpElZaks"
      },
      "source": [
        "# Função que pega uma sequência e transforma em input & target\n",
        "# Ex:\n",
        "# seq = tensorflow\n",
        "# input = tensorflo\n",
        "# target = ensorflow\n",
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivUwwTk8Z1bU"
      },
      "source": [
        "# Aplicar a função nos exemplos (batches) para gerar um dataset de treino:\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWIw3JiXavyJ",
        "outputId": "2cb71765-48b2-4483-a3d6-13f0649a69d9"
      },
      "source": [
        "# Dividir e randomizar\n",
        "\n",
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4orv05DcTOz"
      },
      "source": [
        "# Definição do Modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DhogTrMcaq-"
      },
      "source": [
        "**tf.keras.layers.Embedding:**\n",
        "\n",
        "The input layer. A trainable lookup table that will map each character-ID to a vector with embedding_dim dimensions;\n",
        "\n",
        "**tf.keras.layers.GRU:**\n",
        "\n",
        "A type of RNN with size units=rnn_units (You can also use an **LSTM** layer here.)\n",
        "\n",
        "**tf.keras.layers.Dense:**\n",
        "\n",
        "The output layer, with vocab_size outputs. It outputs one logit for each character in the vocabulary. These are the log-likelihood of each character according to the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPVLeHoYcV7k"
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ucOsLvMc_KO"
      },
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    # Passar o sinal pela camada de embeding\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    # Passar o sinal pela RNN podendo já ter um estado e estando ou não em treino\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    # Finalmente, passar o sinal pela camada densa\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdJ7UPqAePet"
      },
      "source": [
        "model = MyModel(\n",
        "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5lVxTurfi9_"
      },
      "source": [
        "# Rodando o modelo sem treinamento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FfAy47nfiBX",
        "outputId": "f531c99f-2844-40f7-c5bb-4e5e712f9ad3"
      },
      "source": [
        "# Priemiro só checando como fica o output:\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n",
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        multiple                  16896     \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    multiple                  3938304   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                multiple                  67650     \n",
            "=================================================================\n",
            "Total params: 4,022,850\n",
            "Trainable params: 4,022,850\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBLGOVnAhQES",
        "outputId": "d7d7154f-2969-4cec-d269-43d6050edd4b"
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
        "\n",
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b\"hy blood,\\nCongeal'd with this, do make me wipe off both.\\n3 KING HENRY VI\\n\\nYORK:\\nThe army of the quee\"\n",
            "\n",
            "Next Char Predictions:\n",
            " b\"o&psxF;meXXIPHjbFlQkmdAL BrTCW$PA--lN PcJ[UNK]bEltCX:eJyE;k.s[UNK]gGNmpfuQaJgZ;.DTewl?S\\nJnPgW,&KiQXFe',Ovj-s\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Thi-aHS3i0Ic",
        "outputId": "cfa4c1be-a428-470b-9cc1-0532bf483bc4"
      },
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
        "mean_loss = example_batch_loss.numpy().mean()\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", mean_loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         4.1898317\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zH4kjhvixbD"
      },
      "source": [
        "# Treinamento do modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VM6LfQfijR3U"
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSMXPf9NjV7f"
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjnNegOMjcaS"
      },
      "source": [
        "EPOCHS = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAbsARycjeKa",
        "outputId": "6427091f-63c0-4119-8371-fc276de8972e"
      },
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "172/172 [==============================] - 25s 124ms/step - loss: 2.7278\n",
            "Epoch 2/20\n",
            "172/172 [==============================] - 23s 125ms/step - loss: 1.9931\n",
            "Epoch 3/20\n",
            "172/172 [==============================] - 23s 127ms/step - loss: 1.7112\n",
            "Epoch 4/20\n",
            "172/172 [==============================] - 24s 128ms/step - loss: 1.5508\n",
            "Epoch 5/20\n",
            "172/172 [==============================] - 24s 128ms/step - loss: 1.4530\n",
            "Epoch 6/20\n",
            "172/172 [==============================] - 23s 128ms/step - loss: 1.3842\n",
            "Epoch 7/20\n",
            "172/172 [==============================] - 24s 128ms/step - loss: 1.3315\n",
            "Epoch 8/20\n",
            "172/172 [==============================] - 24s 128ms/step - loss: 1.2869\n",
            "Epoch 9/20\n",
            "172/172 [==============================] - 24s 128ms/step - loss: 1.2457\n",
            "Epoch 10/20\n",
            "172/172 [==============================] - 24s 129ms/step - loss: 1.2053\n",
            "Epoch 11/20\n",
            "172/172 [==============================] - 24s 129ms/step - loss: 1.1660\n",
            "Epoch 12/20\n",
            "172/172 [==============================] - 24s 129ms/step - loss: 1.1241\n",
            "Epoch 13/20\n",
            "172/172 [==============================] - 24s 130ms/step - loss: 1.0812\n",
            "Epoch 14/20\n",
            "172/172 [==============================] - 24s 129ms/step - loss: 1.0359\n",
            "Epoch 15/20\n",
            "172/172 [==============================] - 24s 128ms/step - loss: 0.9870\n",
            "Epoch 16/20\n",
            "172/172 [==============================] - 23s 128ms/step - loss: 0.9364\n",
            "Epoch 17/20\n",
            "172/172 [==============================] - 24s 128ms/step - loss: 0.8846\n",
            "Epoch 18/20\n",
            "172/172 [==============================] - 24s 129ms/step - loss: 0.8317\n",
            "Epoch 19/20\n",
            "172/172 [==============================] - 24s 129ms/step - loss: 0.7809\n",
            "Epoch 20/20\n",
            "172/172 [==============================] - 24s 129ms/step - loss: 0.7315\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulO4pPXym2Sb"
      },
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdSmiI5qm5gK"
      },
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFLjQj7Im-Ae"
      },
      "source": [
        "Rodar o onestep em loop para gerar texto:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TBcHly8m8sN",
        "outputId": "60eec926-1083-49ea-b6ed-fad1643e0d91"
      },
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(500):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "Such puttingued graves of us he violent his need,\n",
            "And by the hand of maids be his life.\n",
            "Masters all, good request; here it is.\n",
            "Down with that spring, for, lords on Richard's lord,--\n",
            "\n",
            "OFFBROKE:\n",
            "O Clifford, your spirit, in fear it.\n",
            "\n",
            "LUCIO:\n",
            "I warrant their touchments, be married.\n",
            "\n",
            "ANTONIO:\n",
            "And how my minds to you befate you grace:\n",
            "And I will trius. Siciling!\n",
            "Blunt, the matter,--I must confess or grief on't.\n",
            "\n",
            "MENENIUS:\n",
            "Go to have so many great estame.\n",
            "\n",
            "JULIET:\n",
            "It is not thunder; and so to cut it, d \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 2.8446035385131836\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GyPZUBvoODZ"
      },
      "source": [
        "# Salvar o modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g637xrJqoQQR",
        "outputId": "b2b02622-08ee-4c0e-e6c1-b38c0537c3a0"
      },
      "source": [
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7fdc903c9550>, because it is not built.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as gru_cell_layer_call_and_return_conditional_losses, gru_cell_layer_call_fn, gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses, gru_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: one_step/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: one_step/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnSD_wXAobse"
      },
      "source": [
        "Rodando o modelo salvo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HW3nrrgEoan2",
        "outputId": "b14afa83-a019-4fd5-ff81-29c0f182eb77"
      },
      "source": [
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(500):\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "Howest thy tears and swift? a cunning,\n",
            "I ne'er she were so, death's contexts: 'twere pretteen\n",
            "Which his true quanis.\n",
            "\n",
            "ESCALUS:\n",
            "This is a pather-bawd. They are in a time.\n",
            "Your fellows to seek an house, ewes had lint--\n",
            "For hasty man! would you be heard?\n",
            "\n",
            "EDABESBY:\n",
            "About her vergels are.\n",
            "\n",
            "AUTOLYCUS:\n",
            "If you this wilt that were with honour from my father's\n",
            "Defe in my tent? marry her able then together\n",
            "To die Claudio does from the orcasa wide ait\n",
            "There is no vanian's garments, for my hands\n",
            "Showing it\n"
          ]
        }
      ]
    }
  ]
}